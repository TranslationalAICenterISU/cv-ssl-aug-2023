{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Index","text":""},{"location":"#welcome-to-the-trac-adlrg-mini-course-on","title":"Welcome to the TrAC ADLRG Mini-Course on","text":""},{"location":"#computer-vision-self-supervised-learning","title":"Computer Vision &amp; Self-supervised Learning","text":"<p>Hi all! welcome to TrAC ADLRG Mini-Course on Computer Vision &amp; Self-supervised Learning.   Date: Aug 28th, Sept. 4, 11, and 18, 2023  Time: 4 PM \u2013 6 PM  Venue: Zoom</p>"},{"location":"#before-you-get-started-please-read-our","title":"Before you get started, please read our:","text":"<ol> <li> <p> Code of Conduct</p> </li> <li> <p> Logistics</p> </li> <li> <p> Schedule</p> </li> <li> <p> Tutorial Slides</p> </li> <li> <p> External Resources</p> </li> </ol>"},{"location":"cloud/codespaces/","title":"GitHub CodeSpaces","text":"<p>CodeSpaces are virtual services which provides \"cloud-based development environment\" for software programmers and data scientists. </p> <p>CodeSpaces is run on cloud services (Microsoft Azure), and links with your GitHub account for a seamless experience working on code in a Git repository.</p>"},{"location":"cloud/codespaces/#starting-codespace","title":"Starting CodeSpace","text":"<p>When a GitHub Organization and Repository have CodeSpaces enabled you will see a \"Code\" button above the README.md</p> <p></p> <p>Click on the \"Code\" button and start a new CodeSpace</p> <p></p> <p>Select the size of the CodeSpace you want (2-4 cores and 4GB to 8GB of RAM should be plenty for today)</p> <p></p> <p>Click \"Create CodeSpace\"</p> <p>You will be taken to a loading screen, and after a few moments (&lt;2 minutes) your browser will change to a VS Code instance in your browser.</p> <p></p> <p>Notice, the GitHub repository where you initiated the CodeSpace is set as the working directory of the EXPLORER  in the upper left side of VS Code interface. You're in your Git repo, and are able to work with Python, Docker, Node, or any one of many featured developer tools. Further, you can install any tools you like!</p> <p></p>"},{"location":"cloud/js2/","title":"Jetstream-2","text":"<p>To log into the JupyterHub, go to: <code>http://tractrainXX.cyverse.org/</code> -- you will be assigned a node designated by <code>XX</code>, e.g. <code>01</code>, <code>02</code>, ..., <code>15</code> </p> <p>Use your GitHub credentials to log into the Hub.</p> <p>A Jupyter Lab should open for you.</p>"},{"location":"cloud/js2/#get-access-to-jetstream-2-for-yourself-or-your-research-group","title":"Get Access to Jetstream-2 for yourself or your research group","text":"<p>Jetstream-2 is a public research cloud funded by the National Science Foundation. </p> <p>Access to Jetstream-2 is managed by XSEDE.org</p> <p>To get access to Jetstream-2 you must:</p> <ul> <li> <p>Create a XSEDE Portal Account</p> </li> <li> <p>Request a Start-up Allocation under \"Allocations\" &gt; \"Submit/Review Request\"</p> </li> <li> <p>For Jetstream-2 write a short project description and justification (typically less than 3 pages) </p> </li> </ul> <p>After you have been approved, you will be able to access Jetstream-2 using either their web-based Exosphere interface, </p> <p>or you can explore using other command-line API options from their documentation. </p>"},{"location":"getting_started/code_conduct/","title":"Code of Conduct","text":"<p>All attendees, speakers, staff, and volunteers are required to follow our code of conduct.</p> <p>Iowa State University expects and appreciates cooperation from all participants to help ensure a safe, collaborative environment for everyone. Harrassment by any individual will not be tolerated and may result in the individual being removed from the Camp.</p> <p>Harassment includes: offensive verbal comments related to gender, gender identity and expression, age, sexual orientation, disability, physical appearance, body size, race, ethnicity, religion, technology choices, sexual images in public spaces, deliberate intimidation, stalking, following, harassing photography or recording, sustained disruption of talks or other events, inappropriate physical contact, and unwelcome sexual attention.</p> <p>Participants who are asked to stop any harassing behavior are expected to comply immediately.</p> <p>Workshop staff are also subject to the anti-harassment policy. In particular, staff should not use sexualised images, activities, or other material.</p> <p>If a participant engages in harassing behavior, the workshop organisers may take any action they deem appropriate, including warning the offender or expulsion from the workshop with no refund.</p> <p>If you are being harassed, or notice that someone else is being harassed, or have any other concerns, please contact a member of the workshop staff immediately. Staff can be identified as they\u2019ll be wearing badges or nametags.</p> <p>Workshop staff will be happy to help participants contact local law enforcement, provide escorts, or otherwise assist those experiencing harassment to feel safe for the duration of the workshop. We value your attendance.</p> <p>We expect participants to follow these rules at conference and workshop venues and conference-related social events.</p> <p>See http://www.ashedryden.com/blog/codes-of-conduct-101-faq for more information on codes of conduct.</p>"},{"location":"getting_started/logistics/","title":"Logistics","text":"<p>Date: Aug 28th, Sept. 4, 11, and 18, 2023  Time: 4 PM \u2013 6 PM  Venue: Zoom</p>"},{"location":"getting_started/schedule/","title":"Schedule","text":"<p>All times shown in Central Daylight Time</p> <p>Date: Aug 28th, Sept. 4, 11, and 18, 2023  Time: 4 PM \u2013 6 PM  Venue: Zoom</p> Date(s) Concept Aug 28th, 2023 Computer Vision Sept. 4, 11, and 18, 2023 Self-supervised Learning"},{"location":"sections/m01_intro/","title":"Introductory Exercises with YOLOv8","text":"<p>Download all the notebooks for the following sections to use with Google Colab:</p> <ul> <li>notebooks.zip</li> <li>https://colab.research.google.com/</li> </ul> <p>YOLOv8 is capable of classification, detection, segmentation, and pose.</p> <p>Check out:</p> <ul> <li>https://github.com/ultralytics/ultralytics</li> </ul> <p>Explore these different tasks with YOLOv8:</p> <ul> <li>01_YOLOv8.ipynb</li> </ul>"},{"location":"sections/m02_tracking/","title":"Tracking with YOLOv8","text":"<p>While YOLOv8 has ByteTrack and BoT-SORT without re-identification built-in, the yolo_tracking repo has support for multiple SOTA tracking and re-identification methods.</p> <p>Check out:</p> <ul> <li>https://github.com/mikel-brostrom/yolo_tracking</li> </ul> <p>Perform inference on a sample video:</p> <ul> <li>02_yolo_tracking.ipynb</li> </ul>"},{"location":"sections/m03_clip/","title":"Zero Shot Object Classification with CLIP","text":"<ul> <li>CLIP is a zero shot image classifier released by OpenAI</li> <li>Trained on 400 million text/image pairs across the web</li> </ul> <p>Check out:</p> <ul> <li>https://openai.com/research/clip</li> <li>https://github.com/openai/CLIP</li> <li>https://arxiv.org/abs/2103.00020</li> </ul> <p>Try the model:</p> <ul> <li>03_clip.ipynb</li> </ul>"},{"location":"sections/m04_grounding_dino/","title":"Zero Shot Object Detection with Grounding DINO","text":"<p>Grounding DINO can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector DINO for open-set concept generalization.</p> <p>Check out:</p> <ul> <li>https://github.com/IDEA-Research/GroundingDINO</li> <li>https://arxiv.org/abs/2303.05499</li> </ul> <p>Try detection on a few sample images:</p> <ul> <li>04_grounding_dino.ipynb</li> </ul>"},{"location":"sections/m05_fast_sam/","title":"Fast Segment Anything Model (FastSAM)","text":"<p>FastSAM is a CNN Segment Anything Model trained by only 2% of the SA-1B dataset published by SAM authors. It is able to process images in only 40ms compared to the 2099ms/img of SAM.</p> <p>Check out:</p> <ul> <li>https://github.com/CASIA-IVA-Lab/FastSAM</li> <li>https://arxiv.org/pdf/2306.12156.pdf</li> </ul> <p>Try the model:</p> <ul> <li>05_fast_sam.ipynb</li> </ul>"},{"location":"sections/m06_monodepth/","title":"Monoocular Depth Estimation with MiDaS","text":"<p>Monocular depth estimation estimates the depth in an image using only a single RGB camera as input.</p> <p>Check out:</p> <ul> <li>https://github.com/isl-org/MiDaS</li> <li>https://ieeexplore.ieee.org/document/9178977</li> </ul> <p>Try the model:</p> <ul> <li>06_monodepth.ipynb</li> </ul>"},{"location":"sections/resources/","title":"7. Resources","text":"<ol> <li>Introduction<ol> <li>Tools for generating images using description<ol> <li>https://beta.dreamstudio.ai/generate</li> <li>https://huggingface.co/spaces/stabilityai/stable-diffusion</li> <li>https://www.bing.com/create</li> </ol> </li> <li>Research paper collection<ol> <li>https://aman.ai/papers/</li> </ol> </li> <li>Improving deep learning performance<ol> <li>https://horace.io/brrr_intro.html</li> </ol> </li> <li>Multi-model deep learning<ol> <li>https://arxiv.org/pdf/2301.04856.pdf</li> </ol> </li> <li>AI-based ppt generation<ol> <li>https://slidesgpt.com/index.html</li> </ol> </li> <li>Visualize your dreams<ol> <li>https://beta.dreamstudio.ai/generate</li> </ol> </li> <li>ChatGPT<ol> <li>https://chat.openai.com/auth/login</li> </ol> </li> </ol> </li> <li>Multiple modalities of Data to enhance AI<ol> <li>Computer Vision<ol> <li> <ul> <li>Additional models for classification, pose estimation, depth estimation, and ReID are just gotten from paperswithcode rankings</li> </ul> </li> <li>YOLOv8 is very good for exploring different computer vision tasks since it has detection, segmentation, classification, pose, and tracking (ByteTrack and BoT-SORT without ReID for now) implemented</li> <li>Object detection<ol> <li>CNN-based<ol> <li>Requires relatively little data (especially if pretrained with datasets such as COCO) fast inference</li> <li>YOLOv8 (still no actual paper)<ul> <li>https://github.com/ultralytics/ultralytics </li> <li>https://blog.roboflow.com/whats-new-in-yolov8/ </li> </ul> </li> <li>YOLOv6.3<ul> <li>Situationally better than v8, but it isn't as user-friendly and doesn't have additional functionality (segmentation, classification, pose)</li> <li>https://arxiv.org/abs/2301.05586 </li> </ul> </li> </ol> </li> <li>Transformer-based detection<ol> <li>Generally requires more data and has slower inference but can detect many more types of objects/classes</li> <li>DETR<ol> <li>https://github.com/facebookresearch/detr </li> <li>https://arxiv.org/abs/2005.12872 </li> </ol> </li> <li>Grounding DINO: SOTA Zero-Shot Objection Detection<ol> <li>Can detect new classes with no additional training, one of their examples is a dog's tail</li> <li>https://blog.roboflow.com/grounding-dino-zero-shot-object-detection/</li> <li>https://arxiv.org/abs/2303.05499 </li> </ol> </li> <li>DINO v2<ol> <li>https://ai.facebook.com/blog/dino-v2-computer-vision-self-supervised-learning/ </li> <li>https://arxiv.org/abs/2304.07193</li> </ol> </li> </ol> </li> </ol> </li> <li>Object segmentation<ol> <li>YOLOv8 also has segmentation<ol> <li>Segment anything</li> <li>https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/ </li> <li>https://segment-anything.com/</li> <li>https://huggingface.co/spaces/curt-park/segment-anything-with-clip</li> </ol> </li> </ol> </li> <li>Classification<ol> <li>YOLOv8 also has classification</li> <li>CoCa: Contrastive Captioners<ol> <li>https://arxiv.org/abs/2205.01917</li> </ol> </li> </ol> </li> <li>Pose estimation<ol> <li>YOLOv8 also has pose</li> <li>ViTPose+<ol> <li>https://github.com/vitae-transformer/vitpose </li> <li>https://arxiv.org/abs/2212.04246 </li> <li>https://arxiv.org/abs/2204.12484 </li> </ol> </li> </ol> </li> <li>Tracking<ol> <li>BoT-SORT<ol> <li>https://arxiv.org/abs/2206.14651 </li> </ol> </li> <li>Deep OC-SORT<ol> <li>Better than BoT-SORT in some metrics, and worse in others, but it also runs faster (good for edge devices where the lower fps of BoT-SORT can impact the motion model)</li> <li>https://arxiv.org/abs/2302.11813</li> </ol> </li> <li>Person ReID</li> <li>Fast-ReID: A Pytorch Toolbox for General Instance Re-identification<ol> <li>https://arxiv.org/abs/2006.02631</li> </ol> </li> <li>DenseIL \u2013 Dense Interaction Learning for Video-based Person Re-identification\u00a0<ol> <li>https://arxiv.org/pdf/2103.09013v3.pdf</li> </ol> </li> </ol> </li> <li>Depth estimation<ol> <li>HiMODE<ol> <li>https://arxiv.org/pdf/2204.05007v1.pdf</li> </ol> </li> <li>FreDSNet<ol> <li>https://github.com/sbrunoberenguel/fredsnet</li> <li>https://arxiv.org/pdf/2210.01595v1.pdf</li> </ol> </li> </ol> </li> <li>Many more notebooks for various computer vision tasks<ol> <li>Roboflow Notebooks<ol> <li>https://github.com/roboflow/notebooks</li> </ol> </li> <li>OpenVINO Notebooks<ol> <li>https://github.com/openvinotoolkit/openvino_notebooks</li> </ol> </li> <li>Microsoft Computer Vision Recipes<ol> <li>https://github.com/microsoft/computervision-recipes/tree/staging/scenarios</li> </ol> </li> </ol> </li> <li>Lots of practice exercises<ol> <li>Udacity's Computer Vision Nanodegree program<ol> <li>https://github.com/udacity/CVND_Exercises</li> </ol> </li> </ol> </li> </ol> </li> </ol> </li> <li>Novel Algorithms for enhancing AI<ol> <li>Self-supervised learning<ol> <li>https://arxiv.org/pdf/2304.12210.pdf</li> </ol> </li> </ol> </li> </ol>"}]}